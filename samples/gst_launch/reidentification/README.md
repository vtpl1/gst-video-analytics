# Face Reidentification Sample

This sample demonstrates how to recognize faces on video by comparing face embedding (feature vector) generated by reidentification model against gallery on known faces.

## How It Works
Script `gallery_generor.py` helps to convert images with known faces into .json and .tensor files with feature vectors.

Custom GStreamer element `gvaidentify` implemented on C/C++ and inserted into pipeline after `gvadetect` and `gvaclassify` to compare feature vector aginst gallery and assign name to recognized faces. The `gvawatermark` element visualizes bounding boxes and name of recognized faces.

## Compiling custom element `gvaidentify`
Run the following commands to compile and register custom element
```
cd gvaidentify
cmake .
make
```

## Creating a Gallery for Face Recognition

To recognize faces on a frame, the gallery with features for each person should be created. Gallery can be created using supplied gallery_generator.py script. To do this, you need to create the correct folder with images.
The script uses the **gvametaconvert** to save tensors from **gvaclassify** to a file. Parameter **method** in gvametaconvert is used to create the file name. This file name is defined as:
 
1. image file name - if image is in the root folder
2. subfolder name - if image is in the subfolder
 
Use following command line to run the script:
* **python3 gallery_generator.py -s <images_folder_path>**

After running the script a folder (*./features* by default) with tensors in the file and also a gallery.json file with description will be created.

For example:
Input images folder structure:
* images
  * person_1
    * img1.png
    * img2.png
  * person_2
    * img1.png
    * img2.png
  * person_3
    * img1.png
 
Outputs will be:
* features
  * person1_0_frame_0_idx_0.tensor
  * person1_1_frame_0_idx_0.tensor
  * person2_0_frame_0_idx_0.tensor
  * person2_1_frame_0_idx_0.tensor
  * person3_0_frame_0_idx_0.tensor
* gallery.json

## Models

The sample uses by default the following pre-trained models from OpenVINOâ„¢ [Open Model Zoo](https://github.com/opencv/open_model_zoo)
*   __face-detection-adas-0001__ is primary detection network for finding faces
*   __landmarks-regression-retail-0009__ generates landmark points for each detected face
*   __face-reidentification-retail-0095__ generates face embeddings (feature vector) for each detected face

> **NOTE**: Before running samples (including this one), run script `download_models.sh` once (the script located in `samples` top folder) to download all models required for this and other samples.

## Running

```sh
./face_recognition.sh [INPUT_VIDEO] [GALLERY_FILE.json]
```

If command-line parameter `INPUT_VIDEO` not specified, the sample by default streams video example from HTTPS link (utilizing `urisourcebin` element) so requires internet conection.
The command-line parameter INPUT_VIDEO allows to change input video and supports
* local video file
* web camera device (ex. `/dev/video0`)
* RTSP camera (URL starting with `rtsp://`) or other streaming source (ex URL starting with `http://`)

The `GALLERY_FILE.json` parameter is path to .json file with reference to .tensor files with feature vectors, see section "Creating a Gallery for Face Recognition".

## Sample Output

The sample
* prints gst-launch-1.0 full command line into console
* starts the command and visualizes video with bouding boxes around detected faces and name around recognized faces

## See also
* [DL Streamer samples](../../../README.md)
